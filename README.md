# LLM-AI for EXCLUSIBLE

There are hundreds of LLMs on the market. This repository is a comparative list of the most established.

## Pre-trained LLM

| Name of Model  | Release Type   | License                     | Accessibility                                        | Dataset Used for Training                                                                                                | Parameters                                                                   | Features                                                                                                                                                                             | Author                      |
| -------------- | -------------- | --------------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------- |
| GPT-4          | Commercial     | Paid                        | Limited access via ChatGPT and API (with a waitlist) | 45TB of various, including books, articles and websites                                                                  | 175 billion                                                                  | Almost any NLU, NLG, and multimodal task; advanced reasoning and instruction-following capabilities; improved safety and alignment                                                   | OpenAI                      |
| LLaMA          | Research only  | Open source, Non-Commercial | App required                                         | CCNet, C4, GitHub, Wikipedia, Books, ArXiv, Stack Exchange                                                               | ?                                                                            | Exploring the capabilities and limitations of large language models for various natural language processing tasks                                                                    | Meta AI                     |
| StableLM       | Research only  | CC BY-NC-SA 4.0             | Hugging Face                                         | The Pile                                                                                                                 | 3 billion and 7 billion (alpha version), 15 billion and 65 billion (planned) | High performance in both conversational and coding tasks, with low parameter count and high efficiency                                                                               | Stability AI                |
| Dolly          | Open source    | Apache 2.0                  | Hugging Face or locally                              | 15k instructions/response records generated by Databricks employees                                                      | 12 billion                                                                   | Commercially usable ChatGPT-style AI model developed by Databricks. Trained on a high-quality human-generated instruction following dataset, crowdsourced among Databricks employees | Databricks                  |
| GPT4All        | Open source    | Apache 2.0                  | GitHub                                               | Curated corpus of assistant interactions and publicly available datasets                                                 | 7 billion                                                                    | An assistant-style chatbot that can respond to a wide range of queries with coherent and contextually appropriate responses                                                          | Nomic AI                    |
| Alpaca         | Research only  | CC BY-NC 4.0                | GitHub                                               | 52k examples generated by the Self-Instruct technique                                                                    | 7 billion                                                                    | A strong instruction-following model that can generate natural language texts from instructions                                                                                      | Standford Alpaca project    |
| Vicuna         | Research only  | Non-Commercial              | Hugging Face, GitHub                                 | Fine-tuned on user-shared conversations collected from ShareGPT                                                          | 13 billion                                                                   | A chatbot that can generate natural language texts with high quality and diversity                                                                                                   | LMSYS                       |
| Pythia         | Research only  | Apache 2.0                  | GitHub                                               | ?                                                                                                                        | ?                                                                            | A framework for interpreting autoregressive transformers across time and scale using various methods such as probing, attribution, and visualization                                 | EleutherAI                  |
| T5X            | Open source    | Apache 2.0                  | Hugging Face, GitHub                                 | ?                                                                                                                        | ?                                                                            | A modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models at many scales            | Google Research             |
| UL2            | Open source    | OpenRAIL-M v1               | GitHub                                               | ?                                                                                                                        | ?                                                                            | A modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models at many scales            | Google Research             |
| Cerebras-GPT   | Open source    | Apache 2.0                  | Hugging Face and GitHub                              | Common Crawl                                                                                                             | Varies from 111 million to 13 billion                                        | Compute-efficient, accurate, scalable, data-parallel                                                                                                                                 | Cerebras Systems            |
| BLOOM          | Open source    | Apache 2.0                  | Hugging Face and GitHub                              |  ROOTS corpus                                                                                                            | 176 billion                                                                  | Multilingual, autoregressive, instructable, scalable                                                                                                                                 | BigScience                  |
| FLAN           | Open source    | Apache 2.0                  | ?                                                    | ?                                                                                                                        | 137b                                                                         | Instruction fine-tuning, zero-shot learning, natural language processing                                                                                                             | Google Research             |
| GALACTICA      | Research paper | ?                           | ?                                                    | Scientific articles, websites, textbooks, lecture notes, and encyclopedias                                               | ?                                                                            | Large language model for science                                                                                                                                                     | Meta AI                     |
| GPT-J          | Open source    | Apache 2.0                  | Hugging Face and GitHub                              | The Pile                                                                                                                 | 6 billion                                                                    |  GPT-3-like causal language model, better in code generation                                                                                                                         | EleutherAI                  |
| HuggingGPT     | ?              | ?                           | ?                                                    | ?                                                                                                                        | ?                                                                            | AI that delegates to other AI from HuggingFace (and ChatGPT) platform                                                                                                                | Microsoft                   |
| Polyglot       | Open source    | Apache 2.0                  | GitHub                                               | Various multilingual data                                                                                                | Varies from 1.3 billion to 12.8 billion                                      | Like BLOOM, but better competence in multi-languages                                                                                                                                 | EleutherAI                  |
| GPT-NeoX-20B   | Open source    | Apache 2.0                  | Available on Hugging Face and GitHub                 | The Pile                                                                                                                 | 20 billion                                                                   | Autoregressive language model of general-purpose nature                                                                                                                              | EleutherAI                  |
| Open Assistant | Open source    | Apache 2.0                  | Available on Hugging Face and GitHub                 |  Human demonstrations of assistant conversations collected through the https://open-assistant.io/ human feedback web app | Varies from 161 million to 12 billion                                        |  Chat-based and open-source assistant that understands tasks, can interact with third-party systems and retrieve information dynamically                                             | Open-Assistant Contributors |

## LLM Training Frameworks

| Name            | Link                                   | Feature                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| --------------- | -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Alpa            | https://github.com/alpa-projects/alpa  | system for training and serving large-scale neural networks. Scaling neural networks to hundreds of billions of parameters has enabled dramatic breakthroughs such as GPT-3, but training and serving these large-scale neural networks require complicated distributed system techniques. Alpa aims to automate large-scale distributed training and serving with just a few lines of code.                                                           |
| DeepSpeed Chat  | https://github.com/microsoft/DeepSpeed | DeepSpeed is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for DL Training and Inference.                                                                                                                                                                                                                                                                                                        |
| Megatron-LM     | https://github.com/NVIDIA/Megatron-LM  | large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. This repository is for ongoing research on training large transformer language models at scale.                                                                                                                                                                                                                                                            |
| Colossal-AI     | https://colossalai.org/                | user-friendly tools to kickstart                                                                                                                                                                                                                                                                                                                                                                                                                       |
| BMTrain         | https://github.com/OpenBMB/BMTrain     | efficient large model training toolkit that can be used to train large models with tens of billions of parameters. It can train models in a distributed manner while keeping the code as simple as stand-alone training.                                                                                                                                                                                                                               |
| Mesh TensorFlow | https://github.com/tensorflow/mesh     | language for distributed deep learning, capable of specifying a broad class of distributed tensor computations. The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: "Split the batch over rows of processors and split the units in the hidden layer across columns of processors." Mesh TensorFlow is implemented as a layer over TensorFlow. |

## Useful Resources
- [Emergent Mind](https://chat.lmsys.org/?arena) - Chat with two models side-by-side and vote for which one is better! Includes a Leaderboard.
- [Run ChatGPT-like AI in 60 lines of code](https://jaykmody.com/blog/gpt-from-scratch/#input-%2F-output)

## Sources
- [Open-llms](https://github.com/eugeneyan/open-llms)
- [Awesome-llm](https://github.com/sanjibnarzary/awesome-llm)
- [Open Source ChatGPT like list](https://github.com/SunLemuria/open_source_chatgpt_list)
- [OS Fine-tuned LLMs](https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76)
- [Awesome Open LLM](https://github.com/Hannibal046/Awesome-LLM#open-llm)
