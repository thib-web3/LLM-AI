# LLM-AI

| Name of Model  | Release Type   | License                     | Accessibility                                        | Dataset Used for Training                                                                                                | Parameters                                                                   | Features                                                                                                                                                                             | Author                      |
| -------------- | -------------- | --------------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------- |
| GPT-4          | Commercial     | Paid                        | Limited access via ChatGPT and API (with a waitlist) | 45TB of various, including books, articles and websites                                                                  | 175 billion                                                                  | Almost any NLU, NLG, and multimodal task; advanced reasoning and instruction-following capabilities; improved safety and alignment                                                   | OpenAI                      |
| LLaMA          | Research only  | Open source, Non-Commercial | App required                                         | CCNet, C4, GitHub, Wikipedia, Books, ArXiv, Stack Exchange                                                               | ?                                                                            | Exploring the capabilities and limitations of large language models for various natural language processing tasks                                                                    | Meta AI                     |
| StableLM       | Research only  | CC BY-NC-SA 4.0             | Hugging Face                                         | The Pile                                                                                                                 | 3 billion and 7 billion (alpha version), 15 billion and 65 billion (planned) | High performance in both conversational and coding tasks, with low parameter count and high efficiency                                                                               | Stability AI                |
| Dolly          | Open source    | Apache 2.0                  | Hugging Face or locally                              | 15k instructions/response records generated by Databricks employees                                                      | 12 billion                                                                   | Commercially usable ChatGPT-style AI model developed by Databricks. Trained on a high-quality human-generated instruction following dataset, crowdsourced among Databricks employees | Databricks                  |
| GPT4All        | Open source    | Apache 2.0                  | GitHub                                               | Curated corpus of assistant interactions and publicly available datasets                                                 | 7 billion                                                                    | An assistant-style chatbot that can respond to a wide range of queries with coherent and contextually appropriate responses                                                          | Nomic AI                    |
| Alpaca         | Research only  | CC BY-NC 4.0                | GitHub                                               | 52k examples generated by the Self-Instruct technique                                                                    | 7 billion                                                                    | A strong instruction-following model that can generate natural language texts from instructions                                                                                      | Standford Alpaca project    |
| Vicuna         | Research only  | Non-Commercial              | Hugging Face, GitHub                                 | Fine-tuned on user-shared conversations collected from ShareGPT                                                          | 13 billion                                                                   | A chatbot that can generate natural language texts with high quality and diversity                                                                                                   | LMSYS                       |
| Pythia         | Research only  | Apache 2.0                  | GitHub                                               | ?                                                                                                                        | ?                                                                            | A framework for interpreting autoregressive transformers across time and scale using various methods such as probing, attribution, and visualization                                 | EleutherAI                  |
| T5X            | Open source    | Apache 2.0                  | Hugging Face, GitHub                                 | ?                                                                                                                        | ?                                                                            | A modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models at many scales            | Google Research             |
| UL2            | Open source    | OpenRAIL-M v1               | GitHub                                               | ?                                                                                                                        | ?                                                                            | A modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models at many scales            | Google Research             |
| Cerebras-GPT   | Open source    | Apache 2.0                  | Hugging Face and GitHub                              | Common Crawl                                                                                                             | Varies from 111 million to 13 billion                                        | Compute-efficient, accurate, scalable, data-parallel                                                                                                                                 | Cerebras Systems            |
| BLOOM          | Open source    | Apache 2.0                  | Hugging Face and GitHub                              |  ROOTS corpus                                                                                                            | 176 billion                                                                  | Multilingual, autoregressive, instructable, scalable                                                                                                                                 | BigScience                  |
| FLAN           | Open source    | Apache 2.0                  | ?                                                    | ?                                                                                                                        | 137b                                                                         | Instruction fine-tuning, zero-shot learning, natural language processing                                                                                                             | Google Research             |
| GALACTICA      | Research paper | ?                           | ?                                                    | Scientific articles, websites, textbooks, lecture notes, and encyclopedias                                               | ?                                                                            | Large language model for science                                                                                                                                                     | Meta AI                     |
| GPT-J          | Open source    | Apache 2.0                  | Hugging Face and GitHub                              | The Pile                                                                                                                 | 6 billion                                                                    |  GPT-3-like causal language model, better in code generation                                                                                                                         | EleutherAI                  |
| HuggingGPT     | ?              | ?                           | ?                                                    | ?                                                                                                                        | ?                                                                            | AI that delegates to other AI from HuggingFace (and ChatGPT) platform                                                                                                                | Microsoft                   |
| Polyglot       | Open source    | Apache 2.0                  | GitHub                                               | Various multilingual data                                                                                                | Varies from 1.3 billion to 12.8 billion                                      | Like BLOOM, but better competence in multi-languages                                                                                                                                 | EleutherAI                  |
| GPT-NeoX-20B   | Open source    | Apache 2.0                  | Available on Hugging Face and GitHub                 | The Pile                                                                                                                 | 20 billion                                                                   | Autoregressive language model of general-purpose nature                                                                                                                              | EleutherAI                  |
| Open Assistant | Open source    | Apache 2.0                  | Available on Hugging Face and GitHub                 |  Human demonstrations of assistant conversations collected through the https://open-assistant.io/ human feedback web app | Varies from 161 million to 12 billion                                        |  Chat-based and open-source assistant that understands tasks, can interact with third-party systems and retrieve information dynamically                                             | Open-Assistant Contributors |
